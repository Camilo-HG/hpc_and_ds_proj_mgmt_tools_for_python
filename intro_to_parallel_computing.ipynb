{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf8bf99-7dfe-4958-bb04-a94925ae01ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Brief intro to parallel computing <sup id=\"backref1\"><a href=\"#ref1\">1</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cbf6ad-bf89-42a5-903d-25d0a3430046",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Serial computing\n",
    "\n",
    "The first and straightforward approach in which software is writeen is **serial computing**\n",
    "\n",
    "In this approach, the problem is broken into a discrete number of instructions, which are then executed one after another, and as it only uses one single processor, then only one instruction can be executed at any moment in time.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/serialProblem.gif\" alt=\"Serial problem\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG1\"><a href=\"#ref_IMG1\">IMG 1</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d34ef-b0db-4efc-9a9e-5f750bf89bc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f052c9-ff3d-4a63-bec7-d53b76791155",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the simplest sense, **parallel computing** is the simultaneous use of multiple compute resources to solve a computational problem:\n",
    "\n",
    "- A problem is broken into discrete parts that can be solved concurrently\n",
    "- Each part is further broken down to a series of instructions\n",
    "- Instructions from each part execute simultaneously on different processors\n",
    "- An overall control/coordination mechanism is employed\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/parallelProblem.gif\" alt=\"Serial problem\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG2\"><a href=\"#ref_IMG2\">IMG 2</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b341b3-cda4-4dc5-8bce-587fa970d970",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <img src=\"./images/parallelProblem.gif\" alt=\"Serial problem\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG2\"><a href=\"#ref_IMG2\">IMG 2</a></sup>\n",
    "</center>\n",
    "\n",
    "It is worth to emphasys that parallel computing allows:\n",
    "\n",
    "- To break the problem into muplit parts that can be solved simultaneously.\n",
    "- To execute multiple program instructions at any moment in time.\n",
    "- To solve the problem in less time with multiple compute resources than with a single compute resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc700a64-7dc0-4ed5-b279-8a51c268c285",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A basic concept to understand now, is the difference between a \"single core processor\" and a \"multi-core processor\":\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/singleCore_vs_multiCore.jpg\" alt=\"Single-Core vs. Multi-Core\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG3\"><a href=\"#ref_IMG3\">IMG 3</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1918b-0061-4c1e-89fa-158cfa08e88e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "With this in mind, the compute resources used for parallel computing are typically:\n",
    "\n",
    "- A single computer with multiple processors/cores (**basic parallel computing**)\n",
    "- An arbitrary number of such computers connected by a network (**distributed parallel computing**). Here, each computer is known as a **node**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/parallel-distributed.png\" alt=\"Distributed vs. Parallel (basic)\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG4\"><a href=\"#ref_IMG4\">IMG 4</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a7c82-f57a-4c71-8806-7382cd4b0636",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How to parallelize a program?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025a08a-a9ed-40d3-b462-e96586166b12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Understand the Problem and the Program\n",
    "\n",
    "<br/>\n",
    "\n",
    "Undoubtedly, the first step in developing parallel software is to first understand the problem that you wish to solve in parallel.\n",
    "\n",
    "The first step should be to **start with a serial program** and **understand the serial execution**. Afterwards, you should be able to determine whether or not the problem is one that can actually be parallelized.\n",
    "\n",
    "> $$Programs = algorithms + data + (hardware)$$\n",
    "\n",
    "**NOTE**: Before spending time in an attempt to develop a parallel solution for a problem, you must understand the problem in order to determine if it can be parallelized or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd945e6-5d3f-4e71-9b59-f409055b5459",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Possible approaches\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f4fce-ebd1-44af-80d1-bb57f3de1b08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Identify the program's hotspots**\n",
    "\n",
    "    i.e. _Know where most of the real work is being done_. The majority of scientific and technical programs usually accomplish most of their work in a few places. **Profilers** and performance analysis tools can help here.\n",
    "    \n",
    "    > Focus on parallelizing the hotspots and ignore those sections of the program that account for little CPU usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634216ef-d15a-4a4f-a143-c8274f02fb77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Identify bottlenecks in the program**\n",
    "    \n",
    "    Are there areas that are disproportionately slow, or cause parallelizable work to halt (or be deferred?). For example, I/O is usually something that slows a program down.\n",
    "    \n",
    "    > May be possible to restructure the program or use a different algorithm to reduce or eliminate unnecessary slow areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287678c-be67-470e-b56c-9c02c68cffb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Possible approaches\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0dada4-c052-4496-915f-3d5fbcc39392",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Identify inhibitors to parallelism**\n",
    "\n",
    "    One common class of inhibitor is **data dependence**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae66253-352a-445c-86db-ca8f95ff8678",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "> **Data Dependencies?**\n",
    "> - A **dependence** exists between program statements when the order of statement execution affects the results of the program.\n",
    "> - A **data dependence** results from multiple use of the same location(s) in storage by different tasks.\n",
    "\n",
    "Dependencies are important to parallel programming because they are one of the primary inhibitors to parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694cbcc-5563-4764-89f1-834db193f7de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Possible approaches\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca72d4-e6ac-40e4-8a77-63ac2d04b64c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Investigate other algorithms if possible**\n",
    "\n",
    "    > This may be the single most important consideration when designing a parallel application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34fa0a2-c315-4530-a0b8-609f5139a453",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Problem Decomposition <sup id=\"backref2\"><a href=\"#ref2\">2</a></sup>\n",
    "\n",
    "As we have seen before, the first step in designing a parallel algorithm is to decompose the problem into smaller problems. Then, the smaller problems are assigned to processors to work on simultaneously, i.e., distributed to multiple tasks. This decomposition of the problem is known as **problem decomposition** or **partitioning**.\n",
    "\n",
    "Roughly speaking, there are two kinds of decompositions:\n",
    "\n",
    "1. Domain decomposition\n",
    "2. Functional decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb22b2a-8136-46ce-92e9-262b0d021eda",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Domain Decomposition\n",
    "\n",
    "It is also known as **data parallelism**. In this approach, the data is divided into pieces of approximately the same size and then mapped to different processors. Each processor then works only on the portion of the data that is assigned to it.\n",
    "\n",
    "Of course, the processes may need to communicate periodically in order to exchange data.\n",
    "\n",
    "Data parallelism provides the advantage of maintaining a single flow of control. A data parallel algorithm consists of a sequence of elementary instructions applied to the data: an instruction is initiated only if the previous instruction is ended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905f5ff-18f4-4bf7-b79d-51eb6b2bdf1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Domain Decomposition\n",
    "\n",
    "Ideally:\n",
    "\n",
    "> the code is identical on all processors, so that processors can operate independently on large portions of data, communicating only for few specific tasks.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/domain_decomp.gif\" alt=\"Domain decomposition\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG5\"><a href=\"#ref_IMG5\">IMG 5</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f7cab-a3c3-4b0f-9d0b-b905da7c0a3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Different ways to partition data:\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/data_distributions.gif\" alt=\"Data partition types\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG6\"><a href=\"#ref_IMG6\">IMG 6</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778d839-a090-4c62-8682-5f327d7d127a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Functional Decomposition\n",
    "\n",
    "Frequently, the domain decomposition strategy turns out not to be the most efficient algorithm for a parallel program. This is the case when the pieces of data assigned to the different processes require greatly different lengths of time to process. The performance of the code is then limited by the speed of the slowest process. The remaining idle processes do no useful work.\n",
    "\n",
    "In this case, **functional decomposition** or **task parallelism** makes more sense than domain decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af78f5-6232-4a1e-aaa6-303a98651b8c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Functional Decomposition\n",
    "\n",
    "In this approach, the focus is on the computation that is to be performed rather than on the data manipulated by the computation. The problem is decomposed according to the work that must be done, then these smaller tasks are assigned to the processors as they become available. Processors that finish quickly are simply assigned more work.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/functional_decomp.gif\" alt=\"Functional decomposition\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG7\"><a href=\"#ref_IMG7\">IMG 7</a></sup>\n",
    "</center>\n",
    "\n",
    "Task parallelism is implemented in a **client-server paradigm**: the tasks are allocated to a group of slave processes by a master process that may also perform some of the tasks. The client-server paradigm can be implemented at virtually any level in a program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f7d90-b2c2-4b51-bf07-71225ae8bbf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Communications\n",
    "\n",
    "You must also take into account the design of inter-task communications, but you must be aware that:\n",
    "\n",
    "> The need for communications between tasks depends upon your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e587b-69c4-47cd-9aa8-a8d082a26fce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Communications\n",
    "\n",
    "Some types of problems can be decomposed and executed in parallel with virtually no need for tasks to share data. These types of problems are often called **embarrassingly parallel** - little or no communications are required.\n",
    "\n",
    "For example, imagine an image processing operation where every pixel in a black and white image needs to have its color reversed. The image data can easily be distributed to multiple tasks that then act independently of each other to do their portion of the work.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/black2white.gif\" alt=\"Embarrassingly parallel example\" width=\"250\" height=\"300\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG8\"><a href=\"#ref_IMG8\">IMG 8</a></sup>\n",
    "</center>\n",
    "\n",
    "Most parallel applications are not quite so simple, and do require tasks to share data with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a439be9-c667-404b-9383-6ef5636c7333",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Factors to consider when designing your program's inter-task communications\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73395a2-1f16-4934-a17a-730134065818",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "##### **Communication overhead**\n",
    "\n",
    "Inter-task communication virtually always implies **overhead**, this concept corresponds to the machine cycles and resources that could be used for computation but are instead used to package and transmit data.\n",
    "\n",
    "Communications frequently require some type of synchronization between tasks, which can result in tasks spending time \"waiting\" instead of doing work. In addition, competing communication traffic can saturate the available network bandwidth, further aggravating performance problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda72206-9664-4a0e-a40f-568e11a72566",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Factors to consider when designing your program's inter-task communications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aaf9aa-8922-4533-ac1d-d06d375f21bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "##### **Latency vs. Bandwith**\n",
    "\n",
    "- **Latency** is the time it takes to send a minimal (0 byte) message from point A to point B. Commonly expressed as microseconds.\n",
    "\n",
    "- **Bandwidth** is the amount of data that can be communicated per unit of time. Commonly expressed as megabytes/sec or gigabytes/sec.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/latency_bandwidth.jpg\" alt=\"Latency and Bandwidth\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG9\"><a href=\"#ref_IMG9\">IMG 9</a></sup>\n",
    "</center>\n",
    "\n",
    "Sending many small messages in a pipeline can cause latency to dominate communication overheads, because one must wait each message to be sent in a pipeline. Instead of sending small messages in a pipeline, it is often more efficient to package small messages into a larger message, thus increasing the effective communications bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5df70-9ce1-4ad0-8664-f0bbccf83118",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Factors to consider when designing your program's inter-task communications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752424f-c59f-48ba-be42-2aae489e9ca1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "##### **Synchronous vs. Asynchronous communications**\n",
    "\n",
    "- **Synchronous communications** require some type of \"handshaking\" between tasks that are sharing data. This can be explicitly structured in code by the programmer, or it may happen at a lower level unknown to the programmer. Synchronous communications are often referred to as **blocking communications** since other work must wait until the communications have completed.\n",
    "\n",
    "- **Asynchronous communications** allow tasks to transfer data independently from one another. Asynchronous communications are often referred to as **non-blocking communications** since other work can be done while the communications are taking place. _Interleaving computation with communication is the single greatest benefit for using asynchronous communications_.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/asynchronous-vs-synchronous.png\" alt=\"Sync vs. Async\" width=\"400\" height=\"500\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG10\"><a href=\"#ref_IMG10\">IMG 10</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b6dee-e560-412d-bf62-263fe8f3287b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Factors to consider when designing your program's inter-task communications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee92c99-6b1c-4ad6-8681-6c7c7ea3d666",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "##### **Scope of communications**\n",
    "\n",
    "Knowing which tasks must communicate with each other is critical during the design stage of a parallel code.\n",
    "\n",
    "- **Point-to-point**. Involves two tasks with one task acting as the sender/producer of data, and the other acting as the receiver/consumer.\n",
    "- **Collective**. Involves data sharing between more than two tasks, which are often specified as being members in a common group, or collective. Some common variations (there are more):\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/collective_comm.gif\" alt=\"Collective communications\" width=\"400\" height=\"500\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG11\"><a href=\"#ref_IMG11\">IMG 11</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36d632-f807-4852-b61e-d3e5a4655aa7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Both of the two scopings described can be implemented synchronously or asynchronously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6144cd-4fcc-4ae7-a8ac-72a529a8fc8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Load Balancing\n",
    "\n",
    "**Load balancing** refers to the practice of _distributing approximately equal amounts of work among tasks so that all tasks are kept busy all of the time_. It can be considered:\n",
    "\n",
    "> a minimization of task idle time.\n",
    "\n",
    "Load balancing is important to parallel programs for performance reasons. For example, if all tasks are subject to a barrier synchronization point, the slowest task will determine the overall performance.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/load_balancing.gif\" alt=\"Load Balancing\" width=\"400\" height=\"500\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG12\"><a href=\"#ref_IMG12\">IMG 12</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33559317-d5fd-48ae-816c-43b43f0b3e8d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Certain classes of problems result in load imbalances even if data is evenly distributed among tasks.\n",
    "\n",
    "When the amount of work each task will perform is intentionally variable, or is unable to be predicted, it may be helpful to use a scheduler-task pool approach. As each task finishes its work, it receives a new piece from the work queue.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/schedulerTaskPool.gif\" alt=\"Scheduler - Task Pool\" width=\"500\" height=\"600\">\n",
    "    <br/>\n",
    "    <sup id=\"backref_IMG13\"><a href=\"#ref_IMG13\">IMG 13</a></sup>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb046a-6179-406e-810e-1ffcd4517115",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "<a id=\"ref1\">1</a>: [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref1)\n",
    " \n",
    "<a id=\"ref2\">2</a>: **Introduction to MPI**, PACS Training Group\n",
    "    [↩](#backref2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60870c1-368f-460f-aafd-81b40ae6eaeb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "# Images' references\n",
    "\n",
    "<a id=\"ref_IMG1\">IMG 1</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG1)\n",
    "\n",
    "<a id=\"ref_IMG2\">IMG 2</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG2)\n",
    "\n",
    "<a id=\"ref_IMG3\">IMG 3</a>: Taken from [Certification in Multi-core COTS Modules – When Sharing is a Challenge](https://www.curtisswrightds.com/news/blog/certification-in-multi-core-cots-modules-when-sharing-is-a-challenge.html)\n",
    "    [↩](#backref_IMG3)\n",
    "\n",
    "<a id=\"ref_IMG4\">IMG 4</a>: Adapted from [Parallel versus distributed computing](https://subscription.packtpub.com/book/application_development/9781787126992/1/ch01lvl1sec10/parallel-versus-distributed-computing)\n",
    "    [↩](#backref_IMG4)\n",
    "    \n",
    "<a id=\"ref_IMG5\">IMG 5</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG5)\n",
    "    \n",
    "<a id=\"ref_IMG6\">IMG 6</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG6)\n",
    "\n",
    "<a id=\"ref_IMG7\">IMG 7</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG7)\n",
    "    \n",
    "<a id=\"ref_IMG8\">IMG 8</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG8)\n",
    "    \n",
    "<a id=\"ref_IMG9\">IMG 9</a>: Taken from [Network latency](http://www.techiwarehouse.com/engine/b6fb8338/Network-latency)\n",
    "    [↩](#backref_IMG9)\n",
    "\n",
    "<a id=\"ref_IMG10\">IMG 10</a>: Taken from [Asynchronous vs. Synchronous Programming: When to Use What](https://www.outsystems.com/blog/posts/asynchronous-vs-synchronous-programming/)\n",
    "    [↩](#backref_IMG10)\n",
    "    \n",
    "<a id=\"ref_IMG11\">IMG 11</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG11)\n",
    "    \n",
    "<a id=\"ref_IMG12\">IMG 12</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG12)\n",
    "\n",
    "<a id=\"ref_IMG13\">IMG 13</a>: Taken from [Introduction to Parallel Computing Tutorial](https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial)\n",
    "    [↩](#backref_IMG13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24769a2-ca68-43e6-961f-efed5c6d8505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
